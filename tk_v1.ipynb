{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd430ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import hashlib\n",
    "import torch\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Descargar recursos de NLTK si es la primera vez\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Función para limpiar texto\n",
    "def limpiar_texto(texto):\n",
    "    texto_limpio = re.sub(r'<[^>]+>', ' ', texto)\n",
    "    texto_limpio = texto_limpio.lower()\n",
    "    texto_limpio = re.sub(r'[^a-z0-9áéíóúüñç\\s]', ' ', texto_limpio)\n",
    "    texto_limpio = re.sub(r'\\s+', ' ', texto_limpio).strip()\n",
    "    palabras = nltk.word_tokenize(texto_limpio)\n",
    "    palabras_lematizadas = [lemmatizer.lemmatize(p) for p in palabras]\n",
    "    return ' '.join(palabras_lematizadas)\n",
    "\n",
    "# Cargar archivo JSON de ZAP\n",
    "with open('ReporteZAP.json', 'r', encoding='utf-8') as f:\n",
    "    zap_report = json.load(f)\n",
    "\n",
    "site = zap_report['site'][0]\n",
    "alerts = site['alerts']\n",
    "\n",
    "vuln_data = []\n",
    "for alert in alerts:\n",
    "    cwe_id = alert.get('cweid')\n",
    "    description = alert.get('desc', '')\n",
    "    solution = alert.get('solution', '')\n",
    "    riskdesc = alert.get('riskdesc', '')\n",
    "    uri = alert['instances'][0].get('uri', '') if 'instances' in alert and alert['instances'] else ''\n",
    "    risk = riskdesc.split('(')[0].strip() if riskdesc else ''\n",
    "    vuln_data.append({\n",
    "        'cweid': cwe_id,\n",
    "        'desc': limpiar_texto(description),\n",
    "        'solution': limpiar_texto(solution),\n",
    "        'uri': uri,\n",
    "        'risk': risk\n",
    "    })\n",
    "\n",
    "# Adaptación al formato DiverseVul\n",
    "diverse_data = []\n",
    "project_name = site.get('@host') or site.get('@name') or 'Proyecto_ZAP'\n",
    "\n",
    "for alert, vuln in zip(alerts, vuln_data):\n",
    "    plugin_id = alert.get('pluginid', 'unknown')\n",
    "    commit_id = str(plugin_id)\n",
    "    unique_str = f\"{project_name}_{plugin_id}_{vuln['uri']}\"\n",
    "    hash_id = hashlib.md5(unique_str.encode('utf-8')).hexdigest()[:8]\n",
    "    size = len(vuln['desc'].split())\n",
    "    diverse_data.append({\n",
    "        'func': vuln['desc'],\n",
    "        'target': 1,\n",
    "        'cwe': vuln['cweid'],\n",
    "        'project': project_name,\n",
    "        'commitID': commit_id,\n",
    "        'hash': hash_id,\n",
    "        'size': size,\n",
    "        'message': vuln['solution']\n",
    "    })\n",
    "\n",
    "# Tokenización con T5\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "label_ids_list = []\n",
    "\n",
    "for item in diverse_data:\n",
    "    text = item['func']\n",
    "    label = str(item['cwe'])\n",
    "    enc_in = tokenizer(text, max_length=512, truncation=True, padding='max_length')\n",
    "    enc_out = tokenizer(label, max_length=2, truncation=True, padding='max_length')\n",
    "    input_ids_list.append(enc_in['input_ids'])\n",
    "    attention_mask_list.append(enc_in['attention_mask'])\n",
    "    label_ids = enc_out['input_ids']\n",
    "    label_ids = [(-100 if token_id == tokenizer.pad_token_id else token_id) for token_id in label_ids]\n",
    "    label_ids_list.append(label_ids)\n",
    "\n",
    "input_ids_tensor = torch.tensor(input_ids_list)\n",
    "attention_mask_tensor = torch.tensor(attention_mask_list)\n",
    "labels_tensor = torch.tensor(label_ids_list)\n",
    "\n",
    "print(\"Shape de input_ids:\", input_ids_tensor.shape)\n",
    "print(\"Shape de attention_mask:\", attention_mask_tensor.shape)\n",
    "print(\"Shape de labels:\", labels_tensor.shape)\n",
    "\n",
    "# Cargar modelo para entrenamiento o predicción\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "# Ejemplo de entrada tokenizada\n",
    "ejemplo = diverse_data[0]\n",
    "print(\"\\nEjemplo procesado:\")\n",
    "print(\"Descripción:\", ejemplo['func'])\n",
    "print(\"CWE objetivo:\", ejemplo['cwe'])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
