{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc82ed95",
   "metadata": {},
   "source": [
    "### anexo 1 \n",
    "Con estos campos, podemos extraer la información esencial de cada alerta. A continuación, se muestra cómo cargar el JSON de ZAP y extraer dichos campos en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05f2609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vulnerabilidades extraídas: 17\n",
      "{'cweid': '352', 'desc': \"<p>No Anti-CSRF tokens were found in a HTML submission form.</p><p>A cross-site request forgery is an attack that involves forcing a victim to send an HTTP request to a target destination without their knowledge or intent in order to perform an action as the victim. The underlying cause is application functionality using predictable URL/form actions in a repeatable way. The nature of the attack is that CSRF exploits the trust that a web site has for a user. By contrast, cross-site scripting (XSS) exploits the trust that a user has for a web site. Like XSS, CSRF attacks are not necessarily cross-site, but they can be. Cross-site request forgery is also known as CSRF, XSRF, one-click attack, session riding, confused deputy, and sea surf.</p><p></p><p>CSRF attacks are effective in a number of situations, including:</p><p>    * The victim has an active session on the target site.</p><p>    * The victim is authenticated via HTTP auth on the target site.</p><p>    * The victim is on the same local network as the target site.</p><p></p><p>CSRF has primarily been used to perform an action against a target site using the victim's privileges, but recent techniques have been discovered to disclose information by gaining access to the response. The risk of information disclosure is dramatically increased when the target site is vulnerable to XSS, because XSS can be used as a platform for CSRF, allowing the attack to operate within the bounds of the same-origin policy.</p>\", 'solution': '<p>Phase: Architecture and Design</p><p>Use a vetted library or framework that does not allow this weakness to occur or provides constructs that make this weakness easier to avoid.</p><p>For example, use anti-CSRF packages such as the OWASP CSRFGuard.</p><p></p><p>Phase: Implementation</p><p>Ensure that your application is free of cross-site scripting issues, because most CSRF defenses can be bypassed using attacker-controlled script.</p><p></p><p>Phase: Architecture and Design</p><p>Generate a unique nonce for each form, place the nonce into the form, and verify the nonce upon receipt of the form. Be sure that the nonce is not predictable (CWE-330).</p><p>Note that this can be bypassed using XSS.</p><p></p><p>Identify especially dangerous operations. When the user performs a dangerous operation, send a separate confirmation request to ensure that the user intended to perform that operation.</p><p>Note that this can be bypassed using XSS.</p><p></p><p>Use the ESAPI Session Management control.</p><p>This control includes a component for CSRF.</p><p></p><p>Do not use the GET method for any request that triggers a state change.</p><p></p><p>Phase: Implementation</p><p>Check the HTTP Referer header to see if the request originated from an expected page. This could break legitimate functionality, because users or proxies may have disabled sending the Referer for privacy reasons.</p>', 'uri': 'http://181.54.209.154:8100', 'risk': 'Medium'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Cargar el reporte OWASP ZAP desde un archivo JSON\n",
    "with open('ReporteZAP.json', 'r', encoding='utf-8') as f:\n",
    "    zap_report = json.load(f)\n",
    "\n",
    "# La estructura del JSON tiene un arreglo 'site' con sitios escaneados.\n",
    "# Extraemos el primer (o único) sitio:\n",
    "site = zap_report['site'][0]\n",
    "\n",
    "# Obtener lista de alertas de seguridad\n",
    "alerts = site['alerts']\n",
    "\n",
    "# Inicializar lista para almacenar datos relevantes de cada alerta\n",
    "vuln_data = []\n",
    "\n",
    "for alert in alerts:\n",
    "    # Extraer campos relevantes de la alerta\n",
    "    cwe_id = alert.get('cweid')             # CWE ID (entero, 0 si no tiene)\n",
    "    description = alert.get('desc', '')     # Descripción de la vulnerabilidad (con HTML)\n",
    "    solution = alert.get('solution', '')    # Solución o recomendación sugerida (con HTML)\n",
    "    riskdesc = alert.get('riskdesc', '')    # Riesgo + confianza, ej: \"Medium (Low)\"\n",
    "    uri = '' \n",
    "    # Extraer una URI de ejemplo (la primera instancia de la vulnerabilidad)\n",
    "    if 'instances' in alert and alert['instances']:\n",
    "        uri = alert['instances'][0].get('uri', '')\n",
    "    # Separar el nivel de riesgo del campo combinado (antes del paréntesis)\n",
    "    risk = riskdesc.split('(')[0].strip() if riskdesc else ''\n",
    "\n",
    "    # Almacenar los datos en un diccionario\n",
    "    vuln_data.append({\n",
    "        'cweid': cwe_id,\n",
    "        'desc': description,\n",
    "        'solution': solution,\n",
    "        'uri': uri,\n",
    "        'risk': risk\n",
    "    })\n",
    "\n",
    "# Revisar cuántas vulnerabilidades se extrajeron\n",
    "print(f\"Vulnerabilidades extraídas: {len(vuln_data)}\")\n",
    "# Opcional: mostrar un ejemplo de los datos extraídos (antes de limpieza)\n",
    "print(vuln_data[0])\n",
    "\n",
    "jdown = json.dumps(vuln_data, indent=4, ensure_ascii=False)\n",
    "# Guardar los datos extraídos en un archivo JSON        \n",
    "json_file = 'vul_Norm.json'\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(jdown)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37580575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class ProcesadorReporteZAP:\n",
    "    def __init__(self, archivo_entrada='ReporteZAP.json', archivo_salida='vul_Norm.json'):\n",
    "        self.archivo_entrada = Path(archivo_entrada)\n",
    "        self.archivo_salida = Path(archivo_salida)\n",
    "        \n",
    "    def cargar_reporte(self):\n",
    "        try:\n",
    "            with open(self.archivo_entrada, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error cargando reporte: {str(e)}\")\n",
    "            \n",
    "    def procesar_alerta(self, alert):\n",
    "        return {\n",
    "            'cweid': alert.get('cweid'),\n",
    "            'desc': alert.get('desc', ''),\n",
    "            'solution': alert.get('solution', ''),\n",
    "            'uri': self._obtener_uri(alert),\n",
    "            'risk': self._extraer_nivel_riesgo(alert.get('riskdesc', ''))\n",
    "        }\n",
    "        \n",
    "    def _obtener_uri(self, alert):\n",
    "        return alert['instances'][0].get('uri', '') if 'instances' in alert and alert['instances'] else ''\n",
    "        \n",
    "    def _extraer_nivel_riesgo(self, riskdesc):\n",
    "        return riskdesc.split('(')[0].strip() if riskdesc else ''\n",
    "        \n",
    "    def procesar(self):\n",
    "        try:\n",
    "            zap_report = self.cargar_reporte()\n",
    "            site = zap_report['site'][0]\n",
    "            vuln_data = [self.procesar_alerta(alert) for alert in site['alerts']]\n",
    "            \n",
    "            with open(self.archivo_salida, 'w', encoding='utf-8') as f:\n",
    "                json.dump(vuln_data, f, indent=4, ensure_ascii=False)\n",
    "                \n",
    "            return len(vuln_data)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error procesando reporte: {str(e)}\")\n",
    "\n",
    "# Uso básico\n",
    "procesador = ProcesadorReporteZAP()\n",
    "vulnerabilidades = procesador.procesar()\n",
    "\n",
    "# Uso avanzado con archivos personalizados\n",
    "procesador_custom = ProcesadorReporteZAP(\n",
    "    archivo_entrada='mi_reporte.json',\n",
    "    archivo_salida='resultado_personalizado.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449485c6",
   "metadata": {},
   "source": [
    "Explicación: En este fragmento, se carga el archivo JSON y se itera sobre cada alerta dentro de site['alerts']. Para cada alerta, se obtienen los campos mencionados. Observamos que riskdesc incluye tanto el riesgo como la confianza (por ejemplo \"Medium (Low)\" indica riesgo Medio con confianza Baja​\n",
    "zaproxy.org\n",
    "). Dividimos esa cadena para quedarnos solo con el nivel de riesgo en risk. También obtenemos la primera uri de la lista de instancias donde se encontró la vulnerabilidad. Finalmente, almacenamos los datos en un diccionario por alerta y recopilamos todos en la lista vuln_data. el cual posteriormente se guarda en un archivo json llamado vul_Norm.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba5765",
   "metadata": {},
   "source": [
    "### anexo 2\n",
    "Usaremos bibliotecas Python para estas tareas: re para expresiones regulares (eliminar HTML y caracteres especiales) y nltk para la lematización (WordNet lemmatizer en inglés). Antes de ejecutar, aseguramos tener los recursos de NLTK necesarios (por ejemplo, WordNet). A continuación se presenta una función de limpieza y su aplicación a los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dce5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Descargar recursos necesarios de NLTK (descomentar si es la primera vez)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    # 1. Eliminar etiquetas HTML usando una expresión regular\n",
    "    texto_limpio = re.sub(r'<[^>]+>', ' ', texto)\n",
    "    # 2. Convertir a minúsculas\n",
    "    texto_limpio = texto_limpio.lower()\n",
    "    # 3. Eliminar caracteres especiales (cualquier cosa no alfanumérica o espacio)\n",
    "    texto_limpio = re.sub(r'[^a-z0-9áéíóúüñç\\s]', ' ', texto_limpio)\n",
    "    # 4. Eliminar espacios extra\n",
    "    texto_limpio = re.sub(r'\\s+', ' ', texto_limpio).strip()\n",
    "    # 5. Lematizar cada palabra del texto\n",
    "    palabras = nltk.word_tokenize(texto_limpio)\n",
    "    palabras_lematizadas = [lemmatizer.lemmatize(p) for p in palabras]\n",
    "    texto_limpio = ' '.join(palabras_lematizadas)\n",
    "    return texto_limpio\n",
    "\n",
    "# Aplicar limpieza a la descripción y la solución de cada vulnerabilidad\n",
    "for vuln in vuln_data:\n",
    "    vuln['desc'] = limpiar_texto(vuln['desc'])\n",
    "    vuln['solution'] = limpiar_texto(vuln.get('solution', ''))\n",
    "\n",
    "# Ejemplo: mostrar descripción y solución limpias de la primera vulnerabilidad\n",
    "print(\"Descripción limpia:\", vuln_data[0]['desc'])\n",
    "print(\"Recomendación limpia:\", vuln_data[0]['solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b7547",
   "metadata": {},
   "source": [
    "Explicación: Definimos la función limpiar_texto que aplica todas las transformaciones mencionadas. Primero, reemplazamos cualquier secuencia que luzca como etiqueta HTML por un espacio usando re.sub (regex r'<[^>]+>' captura < seguido de cualquier caracter no > hasta encontrar >). Luego convertimos a minúsculas con .lower(). Usamos otra expresión regular r'[^a-z0-9áéíóúüñç\\s]' para eliminar caracteres no alfanuméricos (permitiendo letras, dígitos, espacios y algunos caracteres con acentos comunes en español). Esto quita signos de puntuación como <, >, /, etc., dejando solo texto limpio. Después contraemos múltiples espacios seguidos en uno solo y hacemos strip() para eliminar espacios iniciales/finales. Finalmente, usamos nltk.word_tokenize para separar el texto en palabras y aplicamos el lematizador de WordNet a cada palabra. La lematización reduce cada palabra a su forma base o lema (por ejemplo, plural a singular, verbos a infinitivo en inglés, etc.), lo cual unifica variantes de la misma palabra​\n",
    ". El texto final se reconstruye uniendo las palabras lematizadas con espacios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c37b7e",
   "metadata": {},
   "source": [
    "### anexo 3 \n",
    "A continuación, construimos la nueva estructura de datos siguiendo estas reglas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab97277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "diverse_data = []  # lista para almacenar las entradas en formato DiverseVul\n",
    "\n",
    "# Obtener identificador del \"proyecto\" (sitio) desde el JSON de ZAP\n",
    "project_name = site.get('@host') or site.get('@name') or 'Proyecto_ZAP'\n",
    "\n",
    "for alert, vuln in zip(alerts, vuln_data):\n",
    "    # Tomar el pluginid de ZAP como base de un ID (es un número string)\n",
    "    plugin_id = alert.get('pluginid', '')\n",
    "    if not plugin_id:\n",
    "        plugin_id = 'unknown'\n",
    "    # Usar pluginid como commitID (en string)\n",
    "    commit_id = str(plugin_id)\n",
    "    # Generar un hash único combinando sitio + pluginid + uri\n",
    "    unique_str = f\"{project_name}_{plugin_id}_{vuln['uri']}\"\n",
    "    hash_id = hashlib.md5(unique_str.encode('utf-8')).hexdigest()[:8]  # hash de 8 caracteres\n",
    "    \n",
    "    # Calcular tamaño como número de palabras de la descripción\n",
    "    size = len(vuln['desc'].split())\n",
    "    \n",
    "    # Armar el diccionario con campos formato DiverseVul\n",
    "    diverse_data.append({\n",
    "        'func': vuln['desc'],             # texto de la \"función\" (descripción de vulnerabilidad)\n",
    "        'target': 1,                      # 1 = vulnerable (todas las alertas lo son)\n",
    "        'cwe': vuln['cweid'],             # ID CWE (entero)\n",
    "        'project': project_name,          # nombre del proyecto/sitio\n",
    "        'commitID': commit_id,            # identificador de commit (simulado con pluginid)\n",
    "        'hash': hash_id,                  # hash único de la entrada\n",
    "        'size': size,                     # tamaño de la \"función\" en número de palabras\n",
    "        'message': vuln['solution']       # mensaje (solución recomendada)\n",
    "    })\n",
    "\n",
    "# Mostrar un ejemplo del nuevo formato\n",
    "from pprint import pprint\n",
    "print(\"Ejemplo de entrada en formato DiverseVul adaptado:\")\n",
    "(diverse_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b2cad",
   "metadata": {},
   "source": [
    "Explicación: Aquí creamos la lista diverse_data donde cada elemento es un diccionario siguiendo el esquema de DiverseVul. Usamos site['@host'] (o @name) para identificar el proyecto (por ejemplo, \"example.com\"). Para commitID, asignamos el pluginid de la alerta (que identifica el tipo de vulnerabilidad en ZAP, por ejemplo 40012 para XSS reflejado). Generamos un hash MD5 a partir del nombre de proyecto, pluginid y URI (concatenados) para obtener un string único (tomamos los primeros 8 caracteres hexadecimales para legibilidad). El campo size es el número de palabras en la descripción limpia. Finalmente, message toma el texto de la solución recomendada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0be92c",
   "metadata": {},
   "source": [
    "### anexo 4\n",
    "Configuración del tokenizador T5: Usaremos la implementación de HuggingFace Transformers. A continuación se muestra cómo inicializar el tokenizador de T5-base y cómo tokenizar un ejemplo de nuestros datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12986f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Inicializar el tokenizador para T5-base\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Tomar un ejemplo de entrada (func) y su etiqueta (cwe) del diverse_data\n",
    "ejemplo = diverse_data[0]\n",
    "input_text = ejemplo['func']        # texto de entrada (descripción de la vulnerabilidad limpia)\n",
    "target_text = str(ejemplo['cwe'])   # texto de la etiqueta (CWE id en string)\n",
    "\n",
    "# Tokenizar la entrada con límite de 512 tokens\n",
    "enc_input = tokenizer(\n",
    "    input_text,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding='max_length',   # rellenar hasta 512 para demostración (opcional, se puede usar padding dinámico en batch)\n",
    "    return_tensors='pt'     # retornar tensores de PyTorch (opcional, para ilustrar)\n",
    ")\n",
    "\n",
    "# Tokenizar la salida/etiqueta con límite de 2 tokens\n",
    "enc_target = tokenizer(\n",
    "    target_text,\n",
    "    max_length=2,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Inspeccionar el resultado de tokenización\n",
    "print(\"Tokens de entrada (ids) [parcial]:\", enc_input['input_ids'][0, :10].tolist(), \"...\")\n",
    "print(\"Longitud total tokens entrada:\", (enc_input['input_ids'] != tokenizer.pad_token_id).sum().item())\n",
    "print(\"Tokens de etiqueta (ids):\", enc_target['input_ids'][0].tolist())\n",
    "print(\"Longitud tokens etiqueta:\", (enc_target['input_ids'] != tokenizer.pad_token_id).sum().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94d612",
   "metadata": {},
   "source": [
    "Explicación: Usamos AutoTokenizer.from_pretrained('t5-base') para cargar el tokenizador preentrenado de T5-base (esto descargará el vocabulario SentencePiece y la configuración del tokenizador). Luego, definimos input_text como el campo func de un ejemplo (que es la descripción de la vulnerabilidad) y target_text como el cwe convertido a string. Tokenizamos la entrada con max_length=512 y truncation=True para asegurarnos de no exceder 512 tokens – si la descripción fuera muy larga, se recortará. Usamos padding='max_length' aquí para rellenar la secuencia a 512 tokens (esto agregará tokens <pad> al final si el texto es más corto). En un caso real, podríamos preferir padding=True (que rellena solo hasta la longitud del texto más largo en el batch) para no añadir padding innecesario; pero para demostración fijamos el tamaño. También pedimos return_tensors='pt' para obtener tensores (aunque podríamos trabajar con las listas de IDs directamente; esto es útil si vamos a usar PyTorch). Repetimos el proceso para el target_text con max_length=2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb1c24",
   "metadata": {},
   "source": [
    "### anexo 5\n",
    "\n",
    "A continuación, convertimos todas las muestras de diverse_data a sus representaciones tokenizadas y preparamos estructuras para entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de225e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar listas para almacenar los tensores/ids de entradas y etiquetas\n",
    "input_ids_list = []\n",
    "attention_mask_list = []\n",
    "label_ids_list = []\n",
    "\n",
    "for item in diverse_data:\n",
    "    # Texto de entrada y texto de salida\n",
    "    text = item['func']\n",
    "    label = str(item['cwe'])\n",
    "    # Tokenizar entrada\n",
    "    enc_in = tokenizer(text, max_length=512, truncation=True, padding='max_length')\n",
    "    # Tokenizar etiqueta\n",
    "    enc_out = tokenizer(label, max_length=2, truncation=True, padding='max_length')\n",
    "    # Extraer ids y mask\n",
    "    input_ids_list.append(enc_in['input_ids'])\n",
    "    attention_mask_list.append(enc_in['attention_mask'])\n",
    "    # Para las etiquetas, reemplazar pad token id por -100 (ignorado en loss) si es necesario\n",
    "    label_ids = enc_out['input_ids']\n",
    "    # Reemplazar <pad> (id=0 en T5) con -100 en la etiqueta\n",
    "    label_ids = [(-100 if token_id == tokenizer.pad_token_id else token_id) for token_id in label_ids]\n",
    "    label_ids_list.append(label_ids)\n",
    "\n",
    "# Convertir a tensores de PyTorch (opcional) para uso directo en modelos/entrenador\n",
    "import torch\n",
    "input_ids_tensor = torch.tensor(input_ids_list)\n",
    "attention_mask_tensor = torch.tensor(attention_mask_list)\n",
    "labels_tensor = torch.tensor(label_ids_list)\n",
    "\n",
    "print(\"Shape de input_ids:\", input_ids_tensor.shape)\n",
    "print(\"Shape de attention_mask:\", attention_mask_tensor.shape)\n",
    "print(\"Shape de labels:\", labels_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a7e48",
   "metadata": {},
   "source": [
    "Explicación: Recorremos cada elemento de diverse_data y aplicamos el tokenizador. En este caso, usamos el tokenizador en modo no-tensor (por defecto devuelve listas de IDs) para luego convertir todo junto a tensores al final. Para cada entrada, truncamos/pad a 512 tokens. Para cada etiqueta, truncamos/pad a 2 tokens. Al obtener los input_ids de la etiqueta, hacemos una post-procesamiento: reemplazamos cualquier token ID que corresponda a <pad> por -100. Esto sigue la convención de training en HuggingFace donde la pérdida ignora posiciones con etiqueta -100. Como en T5 el token de padding tiene id 0, estaremos sustituyendo esos 0 en la secuencia de la etiqueta. Luego, almacenamos las listas de IDs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
